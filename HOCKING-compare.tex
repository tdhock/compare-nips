\documentclass{article}
\usepackage{nips13submit_e,times}
\usepackage{listings}
\usepackage{clrscode}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{slashbox}
\usepackage{multirow}
\usepackage[cm]{fullpage}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{stfloats}
\usepackage{float}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}
\newtheorem{theo}{Theorem}    % numérotés par section
\newtheorem{lemma}{Lemma}

\newcommand{\RR}{\mathbb R}
\newcommand{\NN}{\mathbb N}
\newcommand{\pkg}[1]{\texttt{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}


\newfloat{Algorithm}{thp}{lop}
\floatname{Algorithm}{Algorithm}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\begin{document}
\nipsfinalcopy
\renewcommand{\arraystretch}{1.5}

\definecolor{lightgray}{rgb}{0.9,0.9,0.9}
\definecolor{pastelblue}{RGB}{213,229,255}
\newcolumntype{a}{>{\columncolor{lightgray}}c}

\title{Support vector comparison machines}

\author{
Toby Dylan Hocking \\
Department of Computer Science\\
Tokyo Institute of Technology, Japan \\
\texttt{toby@sg.cs.titech.ac.jp} \\
\And
Supaporn Spanurattana \\
Department of Computer Science\\
Tokyo Institute of Technology, Japan \\
\texttt{supaporn@sg.cs.titech.ac.jp} \\
\And
Masashi Sugiyama \\
Department of Computer Science\\
Tokyo Institute of Technology, Japan \\
\texttt{sugi@cs.titech.ac.jp} 
}

\maketitle

\begin{abstract}
  In ranking problems, the goal is to learn a ranking function
  $r(x)\in\RR$ from labeled pairs $x,x'$ of input points. In this
  paper, we consider the related comparison problem, where the label
  $y\in\{-1,0,1\}$ indicates which element of the pair is better, or
  if there is no significant difference. We cast the learning problem
  as a margin maximization, and show that it can be solved by
  converting it to a standard SVM.
\end{abstract}

\section{Introduction}

In this paper we consider the supervised comparison problem. Assume
that we have $n$ labeled training pairs and for each pair
$i\in\{1,\dots,n\}$ we have input features $x_i,x_i'\in\RR^p$ and a
label $y_i\in\{-1,0,1\}$ that indicates which element is better:
\begin{equation}
  \label{eq:z}
  y_i =
  \begin{cases}
    -1 & \text{ if $x_i$ is better than $x'_i$},\\
    0 & \text{ if $x_i$ is as good as $x'_i$},\\
    1 & \text{ if $x'_i$ is better than $x_i$}.
  \end{cases}
\end{equation}
Comparison data naturally arise when considering subjective human
evaluations of pairs of items. For example, if I were to compare some
pairs of movies I have watched, I would say \textit{Les Mis\'erables}
is better than \textit{Star Wars}, and \textit{The Empire Strikes
  Back} is as good as \textit{Star Wars}. Features $x_i,x_i'$ of the
movies can be length in minutes, year of theatrical release,
indicators for genre, actors, etc.

The goal of learning is to find a comparison function $c:\RR^p \times
\RR^p \rightarrow \{-1,0,1\}$ which generalizes to a test set of data:
\begin{equation}
  \minimize_{c} 
  \sum_{i=1}^n
  e\left[ c(x_i, x_i'), y_i \right],
\end{equation}
where $e(\hat y, y) = I(\hat y \neq y)$ is the zero-one loss.

The rest of this article is organized as follows. In
Section~\ref{sec:related} we discuss links with related work on
classification and ranking, then in Section~\ref{sec:svm-compare} we
propose a new algorithm for comparison. We show results on some toy
data in Section~\ref{sec:results} and discuss future work in
Section~\ref{sec:conclusions}.

\section{Related work}
\label{sec:related}

First we discuss connections with several existing methods in the
machine learning literature, and then we discuss how ranking
algorithms can be applied to the comparison problem. A summary of
related work appears in Table~\ref{tab:related}.

\begin{table}[b!]
  \centering
  \begin{tabular}{|a|c|c|}\hline
    \rowcolor{lightgray}
    \backslashbox{Outputs}{Inputs}
    &single items $x$&pairs $x,x'$\\ \hline
    $y\in\{-1,1\}$ &SVM  & SVMrank   	\\ \hline 
    $y\in\{-1,0,1\}$ &SVM with reject option& this work\\ \hline
  \end{tabular}
  \caption{\label{tab:related} Summary of related max-margin learning methods. 
    Comparison is similar to ranking 
    and classification with reject option.}
\end{table}

\citet{reject-option} studied the statistical properties of a hinge
loss for an extension of SVM where $y\in\{-1,0,1\}$, and
0 signifies ``rejection'' or ``no guess,'' calling this the
``classification with reject option'' problem.
There are many algorithms for the supervised learning to rank problem
\citep{learning-to-rank}, which is similar to the supervised
comparison problem we consider in this paper. The main idea of
learning to rank is to train on labeled pairs of possible documents
$x_i,x_i'$ for the same search query. The labels are $y_i\in\{-1,1\}$,
where $y_i=1$ means document $x_i'$ is more relevant than $x_i$ and
$y_i=-1$ means the opposite. \citet{ranksvm} proposed the SVMrank
algorithm for this problem, and the algorithm we propose here is very
similar. The difference is that we also consider the case where both
possible items/documents are judged to be equally good ($y_i=0$).
\citet{rank-with-ties} proposed a boosting algorithm for this ranking
with ties problem, and observed that ties are more effective when
there are more output values.
\citet{trueskill} proposed TrueSkill, a generalization of the Elo
chess rating system which can be applied to comparison data. However,
neither TrueSkill nor SVMrank can predict ties ($y_i=0$), which is
what motivates us to propose a new algorithm.

% \begin{figure}
%   \centering
%   \input{figure-norm-data}
%   \vskip -0.5cm
%   \caption{Geometric interpretation of the comparison
%     problem. \textbf{Top}: input feature pairs $x_i,x_i'\in\RR^p$ are
%     drawn as segments and arrows, colored using the labels
%     $y_i\in\{-1,0,1\}$. The level curves of the underlying ranking
%     function $r(x)=||x||_2^2$ are drawn in grey, and differences
%     $|r(x)-r(x')|\leq 1$ are considered insignificant
%     ($y_i=0$). \textbf{Middle}: in the enlarged feature space, the
%     ranking function is linear: $r(x)=w^\intercal
%     \Phi(x)$. \textbf{Bottom}: two symmetric hyperplanes
%     $w^\intercal[\Phi(x_i')-\Phi(x_i)]\in\{-1,1\}$ are used as a
%     comparison function to separate the difference vectors.}
%   \label{fig:geometry}
% \end{figure}

% \subsection{SVMrank for comparing}

% In this section we explain how to apply the existing SVMrank algorithm
% of \citet{ranksvm} to a comparison data set.

% The goal of the SVMrank algorithm is to learn a ranking function
% $r:\RR^p \rightarrow \RR$. When $r(x)=w^\intercal x$ is linear, the
% primal problem for some cost parameter $C\in\RR^+$ is the following
% quadratic program (QP):
% \begin{equation}
%   \begin{aligned}
%     \minimize_{w, \xi}\ \  & \frac 1 2 w^\intercal w 
%     + C \sum_{i\in I_1\cup I_{-1}} \xi_i \\
%     \text{subject to}\ \  & 
%     \forall i\in I_1\cup I_{-1},\ \xi_i \geq 0,\\
% &    \text{and }\xi_i \geq 1-w^\intercal(x_i'-x_i)y_i,
%   \end{aligned}
%   \label{eq:svmrank}
% \end{equation}
% where $I_y=\{i\mid y_i=y\}$ are the sets of indices for the different
% labels. Note that the equality pairs $i$ such that $y_i=0$ are not
% used in the optimization problem.

% After obtaining an optimal $w\in\RR^p$ by solving (\ref{eq:svmrank}),
% we define a comparison function $c_t:\RR^p\times \RR^p\rightarrow
% \{-1, 0, 1\}$ for any threshold $t\in\RR^+$:
% \begin{equation}
%   \label{eq:svmrank_c_t}
%   c_t(x, x') =
%   \begin{cases}
%     -1 & \text{ if } w^\intercal(x' - x) < -t, \\
%     0 & \text{ if } |w^\intercal(x' - x)| \leq t, \\
%     1 & \text{ if } w^\intercal(x' - x) > t. \\
%   \end{cases}
% \end{equation}
% We can then use grid search to estimate an optimal threshold $\hat t$,
% by minimizing the zero-one loss with respect to all the training
% pairs:
% \begin{equation}
%   \hat t = \argmin_{t}
%   \sum_{i=1}^n
%   e\left[ c_t(x_i, x_i'), y_i \right].
% \end{equation}
% However, there are two potential problems with the learned comparison
% function $c_{\hat t}$. First, the equality pairs $i\in I_0$ are not
% used to learn the weight vector $w$ in (\ref{eq:svmrank}). Second, the
% threshold $\hat t$ is learned in a separate optimization step, which
% may be suboptimal. In the next section, we propose a new algorithm
% that fixes these potential problems.

\section{Support vector comparison machines}
\label{sec:svm-compare}

In this section we new learning algorithms for comparison problems. In
all cases, we will first learn a ranking function
$r:\RR^p\rightarrow\RR$ and then make a prediction with the comparison
function $c$, defined in terms of the threshold function
$t:\RR\rightarrow\{-1,0,1\}$:
\begin{equation}
  \label{eq:compare_threshold}
  c(x, x') =
  t\left[
    r(x')-r(x)
  \right]
  \begin{cases}
    -1 & \text{if } r(x') - r(x) < -1, \\
    0 & \text{if } |r(x') - r(x)| \leq 1, \\
    1 & \text{if } r(x') - r(x) > 1. \\
  \end{cases}
\end{equation}

\subsection{LP and QP for separable data}
\label{sec:lp-qp}

To illustrate the nature of the max-margin comparison problem, in this
section we assume that the training data are linearly separable. Later
in Section~\ref{sec:kernelized-qp}, we propose an algorithm for
learning a nonlinear function from non-separable data.

In the following linear program (LP), we consider learning a linear
ranking function $r(x)=w^\intercal x$ that maximizes the margin $\mu$:
\begin{equation}
  \label{eq:max-margin-lp}
  \begin{aligned}
    \maximize_{\mu\in\RR, w\in\RR^p}\ & \mu \\
    \text{subject to}\ & \mu \leq 1-|w^\intercal (x_i' - x_i)|,\
    \forall\  i\in I_0,\\
    &\mu \leq -1 +  w^\intercal(x_i'-x_i)y_i,\ \forall\ i\in I_1\cup I_{-1}.
  \end{aligned}
\end{equation}
Note that solving this LP is a test of linear separability. If the
optimal $\mu>0$ then the data are linearly separable. The geometric
interpretation of the LP margin is shown in the left panel of
Figure~\ref{fig:hard-margin}. It is the distance from any difference
vector $x_i'-x_i$ to its nearest decision boundary $r(x)\in\{-1,1\}$.

\begin{figure*}[b!]
  \centering
  \input{figure-hard-margin}
  \vskip -0.5cm
  \caption{The separable LP and QP comparison problems. \textbf{Left}:
    the difference vectors $x'-x$ of the original data and the optimal
    solution to the LP (\ref{eq:max-margin-lp}). \textbf{Middle}: for
    the unscaled flipped data $\tilde x'-\tilde x$ (\ref{eq:tilde}),
    the LP is not the same as the QP
    (\ref{eq:max-margin-qp-tilde}). \textbf{Right}: for the scaled flipped
    data, the QP is equivalent to the LP.}
  \label{fig:hard-margin}
\end{figure*}

Instead of solving this LP, we can instead perform a change of
variables, and then solve a QP to learn a binary SVM. The idea is to
maximize the margin between significant differences $y_i\in\{-1,1\}$
and equality pairs $y_i=0$. Let $X_y,X_y'$ be the $|I_y|\times p$
matrices formed by all the pairs $i\in I_y$. We define a
``flipped'' data set with $m=|I_1|+|I_{-1}|+2|I_0|$ pairs suitable for
training a binary SVM:
\begin{equation}
\label{eq:tilde}
  \tilde X = \left[
    \begin{array}{c}
      X_1 \\
      X_{-1}'\\
      X_0\\
      X_0'
    \end{array}
  \right],\ 
  \tilde X' = \left[
    \begin{array}{c}
      X_1' \\
      X_{-1}\\
      X_0'\\
      X_0
    \end{array}
  \right],\ 
  \tilde y = \left[
    \begin{array}{c}
      1_{|I_1|} \\
      1_{|I_{-1}|}\\
      -1_{|I_0|}\\
      -1_{|I_0|}
    \end{array}
  \right],
\end{equation}
where $1_n$ is an $n$-vector of ones, $\tilde X,\tilde
X'\in\RR^{m\times p}$ and $\tilde y\in\{-1,1\}^m$. Note that $\tilde
y_i=-1$ implies no significant difference between $\tilde x_i$ and
$\tilde x_i'$, and $\tilde y_i=1$ implies that $\tilde x_i$ is better
than $\tilde x_i'$. We then learn an affine function
$f(x)=\beta+u^\intercal x$ using hard-margin binary SVM
\begin{equation}
  \label{eq:max-margin-qp-tilde}
  \begin{aligned}
    \minimize_{u\in\RR^p, \beta\in\RR}\ & u^\intercal u  \\
    \text{subject to}\ & 
    \tilde y_i (\beta + u^\intercal( \tilde x_i'-\tilde x_i) ) \geq 1,
    \ \forall i\in\{1,\dots,m\},
  \end{aligned}
\end{equation}
and define the learned ranking function as $r(x) = -u^\intercal x/\beta$.
\subsection{Kernelized QP for non-separable data}
\label{sec:kernelized-qp}
In this section, we assume the data are not separable, and want to
learn a general nonlinear ranking function. We define a positive
definite kernel $\kappa:\RR^p\times \RR^p\rightarrow\RR$, which
implicitly defines features $\Phi(x)$. As in
(\ref{eq:max-margin-qp-tilde}), we learn a function $f(x)=\beta +
u^\intercal \Phi(x)$ which is affine in the feature space. Let
$\alpha,\alpha'\in\RR^m$ be coefficients such that $u=\sum_{i=1}^m
\alpha_i \Phi(\tilde x_i) + \alpha_i' \Phi(\tilde x_i')$, and so we
have
%the learned function is thus
 $f(x) =\beta + \sum_{i=1}^m \alpha_i \kappa(\tilde x_i, x) +
\alpha_i' \kappa(\tilde x_i', x)$. We then 
define the ranking function
\begin{equation}
  \label{eq:kernelized_r}
  r(x)= \frac{u^\intercal \Phi(x)}{-\beta} = \frac{1}{-\beta}
  \sum_{i=1}^m
    \alpha_i \kappa(\tilde x_i, x) + \alpha_i'  \kappa(\tilde x_i', x).
\end{equation}

Let $K=[K_1\cdots K_m\ K_1'\cdots K_m']\in\RR^{2m\times 2m}$ be the
kernel matrix, where for all $i\in\{1, \dots, m\}$, the kernel
vectors $K_i,K_i'\in\RR^{2m}$ are defined as
\begin{eqnarray}
  K_i &=& \left[
    \begin{array}{cccccc}
      \kappa(\tilde x_1, \tilde x_i)&
      \cdots&
      \kappa(\tilde x_m, \tilde x_i)&
      \kappa(\tilde x_1', \tilde x_i)&
      \cdots&
      \kappa(\tilde x_m', \tilde x_i)
    \end{array}
  \right]^\intercal \\
  K_i' &=& \left[
    \begin{array}{cccccc}
      \kappa(\tilde x_1, \tilde x_i')&
      \cdots&
      \kappa(\tilde x_m, \tilde x_i')&
      \kappa(\tilde x_1', \tilde x_i')&
      \cdots&
      \kappa(\tilde x_m', \tilde x_i')
    \end{array}
  \right]^\intercal.
\end{eqnarray}
Letting $a=[\alpha^\intercal\
\alpha'^\intercal]^\intercal\in\RR^{2m}$, the norm of the linear
function in the feature space is $w^\intercal w = a^\intercal K a$,
and we can write the primal soft-margin comparison QP for some
$C\in\RR^+$ as
\begin{equation}
  \begin{aligned}
      \minimize_{a\in\RR^{2m},\xi\in\RR^m,\beta\in\RR}\ \ & 
      \frac 1 2 a^\intercal K a + C\sum_{i=1}^m \xi_i \\
      \text{subject to}\ \ & 
      \text{for all $i\in\{1,\dots,m\}$, }
      \xi_i \geq 0,\\
      &\text{and }
      \xi_i \geq 1-\tilde y_i(\beta + a^\intercal (K_i'-K_i)).
  \end{aligned}
\end{equation}
Let $\lambda, v\in\RR^m$ be the dual variables, let $Y=\Diag(\tilde
y)$ be the diagonal matrix of $m$ labels. Then the Lagrangian can be
written as
\begin{equation}
  \label{eq:lagrangian}
  \mathcal L = \frac 1 2 a^\intercal K a + C\xi^\intercal 1_{m}\\
  -\lambda^\intercal \xi + v^\intercal(1_m - \tilde y\beta - Y M^\intercal K a - \xi),
\end{equation}
where $M=[-I_m \, I_m]^\intercal\in\{-1,0,1\}^{2m\times m}$. Solving
$\nabla_a \mathcal L=0$ results in the following stationary condition:
\begin{equation}
  \label{eq:stationarity}
  a = M Y v.
\end{equation}
The rest of the derivation of the dual comparison problem is the same
as for the standard binary SVM. The resulting dual QP is
\begin{equation}
  \begin{aligned}
    \label{eq:svm-dual}
    \minimize_{v\in\RR^m}\ \ &
    \frac 1 2 v^\intercal Y M^\intercal K M Y v - v^\intercal 1_m\\
    \text{subject to}\ \ &
    \sum_{i=1}^m v_i \tilde y_i = 0,\\
&    \text{for all $i\in\{1,\dots,m\}$, } 0\leq v_i\leq C,
  \end{aligned}
\end{equation}
which is equivalent to the dual problem of a standard binary SVM with
kernel $\tilde K = M^\intercal K M\in\RR^{m\times m}$ and labels
$\tilde y=\{-1,1\}^m$.

So we can solve the dual problem (\ref{eq:svm-dual}) using
any efficient SVM solver, such as libsvm \citep{libsvm}. We used the R
interface in the \texttt{kernlab} package of \citet{kernlab}, and our
code is available in the \texttt{rankSVMcompare} package on Github.

After obtaining optimal dual variables $v\in\RR^m$ as the solution of
(\ref{eq:svm-dual}), the SVM solver also gives us the optimal bias
$\beta$ by analyzing the complementary slackness conditions.  The
learned ranking function (\ref{eq:kernelized_r}) can be quickly
evaluated since the optimal $v$ is sparse.

The overall training procedure has two hyper-parameters to tune: the
cost $C$ and the kernel $\kappa$. As with standard SVM for binary
classification, these parameters can be tuned by minimizing the
prediction error on a held-out validation set.

% \begin{algorithm}[b!]
%    \caption{\proc{SVMcompare}}
%    \label{alg:SVMcompare}
% \begin{algorithmic}
%   \STATE {\bfseries Input:} cost $C\in\RR^+$, kernel
%   $\kappa:\RR^p\times \RR^p \rightarrow \RR$, features $X,X'\in\RR^{n \times p}$,
%   labels $y\in\{-1,0,1\}^n$.

%   \STATE \makebox[0.5cm]{$\tilde X$} $\gets [$
%   \makebox[1cm]{$X_1^\intercal$}
%   \makebox[1cm]{$X_{-1}'^\intercal$}
%   \makebox[1cm]{$X_0^\intercal$}
%   \makebox[1cm]{$X_0'^\intercal$}
%   $]^\intercal$.

%   \STATE \makebox[0.5cm]{$\tilde X'$} $\gets [$
%   \makebox[1cm]{$X_1'^\intercal$}
%   \makebox[1cm]{$X_{-1}^\intercal$}
%   \makebox[1cm]{$X_0'^\intercal$}
%   \makebox[1cm]{$X_0^\intercal$}
%   $]^\intercal$.

%   \STATE \makebox[0.5cm]{$\tilde y$} $\gets [$
%   \makebox[1cm]{$1_{|I_1|}^\intercal$}
%   \makebox[1cm]{$1_{|I_{-1}|}^\intercal$}
%   \makebox[1cm]{$-1_{|I_0|}^\intercal$}
%   \makebox[1cm]{$-1_{|I_0|}^\intercal$}
%   $]^\intercal$.

%   \STATE $K \gets \proc{KernelMatrix}(\tilde X, \tilde X', \kappa)$.

%   \STATE $M \gets [ -I_m\ I_m ]^\intercal$.

%   \STATE $\tilde K \gets M^\intercal K M$.

%   \STATE $v,\beta \gets \proc{SVMdual}(\tilde K, \tilde y, C)$.

%   % \STATE $\alpha_i \gets $ 
%   % \makebox[1cm][r]{$-\tilde y_i v_i$},
%   % $\forall i\in\{1,\dots, m\}$.

%   % \STATE $\alpha_i' \gets $
%   % \makebox[1cm][r]{$\tilde y_i v_i$},
%   % $\forall i\in\{1,\dots, m\}$.

%   \STATE $\textbf{sv}\gets\{i: v_i>0\}$.
  
%   \STATE {\bfseries Output:} Support vectors $\tilde
%   X_{\textbf{sv}},\tilde X_{\textbf{sv}}'$, labels $\tilde y_{\textbf{sv}}$,
%   bias~$\beta$, dual variables $v$.

%    \end{algorithmic}
% \end{algorithm}

\section{Results}
\label{sec:results}

We used a simulation to compare the proposed comparison model with a
baseline model, SVMrank \citep{ranksvm}. We generated pairs
$x_i,x_i'\in[-3,3]^2$ and noisy labels
$y_i=t[r(x'_i)-r(x_i)+\epsilon_i]$, where $t$ is the threshold
function (\ref{eq:compare_threshold}), $r$ is the latent ranking
function, and $\epsilon_i\sim N(0, 1/4)$ is noise. We picked one train
and one validation set, each with 100 equality pairs ($y_i=0$) and 100
inequality ($y_i\in\{-1,1\}$) pairs, fitting 90 models to the training
set ($C=4^{-4},\dots,4^5$, gaussian kernel width
$\gamma=2^{-7},\dots,2^1$). For the SVMrank model, equality pairs are
ignored when learning the ranking function, but used to learn a
threshold via grid search for when to predict $c(x,x')=0$. The model
with minimal zero-one loss on the validation set is plotted in
Figure~\ref{fig:norms}. The comparison model is better at recovering
the underlying ranking function, since it is able to directly learn
from the equality pairs ($y_i=0$).

\begin{figure}[b!]
  \centering
  \input{figure-simulation}
  \caption{Application to simulated patterns $x,x'\in\RR^2$. The three
    latent ranking functions $r(x)=||x||^2$ are squared norms. In each
    case, half of the training data were equality pairs, which are
    used directly by the comparison model, but are ignored by the
    ranking model.  We draw the level curves of the learned ranking
    functions.  It is clear that the comparison model recovers the
    latent pattern better than the ranking model.}
  \label{fig:norms}
\end{figure}

% \begin{table}[b!]
%   \centering
%   \begin{tabular}{|a|c|c|c|}\hline
%     \rowcolor{lightgray}
%     \backslashbox{$\hat{y}$}{ $y$}
%     &\textbf{-1}&\textbf{0}&\textbf{1}\\ \hline
%     \textbf{-1}&0  & FP & Inversion   	\\ \hline 
%     \textbf{0} &FN& 0& FN\\ \hline
%     \textbf{1} & Inversion & FP &0	\\ \hline
%   \end{tabular}
%   % \cellcolor{pastelblue}
%   \caption{We use the zero-one loss to evaluate a predicted label
%     $\hat y$ given the true label $y$. False positives (FP) occur 
%     when predicting a significant difference $\hat y\in\{-1,1\}$ 
%     when there is none $y=0$, and False Negatives (FN) are the opposite.
%   Inversions occur when predicting the opposite of the true label
%   $\hat y = -y$.}
%   \label{tab:evaluation}
% \end{table}

\section{Conclusions and future work}
\label{sec:conclusions}

We discussed an extension of SVM to comparison problems. Our results
highlighted the importance of directly modeling the equality pairs
($y_i=0$), and it will be interesting to see if the same results are
observed in learning to rank data sets. For scaling to very large data
sets, it will be interesting to try algorithms based on smooth
discriminative loss functions, such as stochastic gradient descent
with a logistic loss.

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
