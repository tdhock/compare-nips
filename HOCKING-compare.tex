\documentclass{article}
\usepackage{nips13submit_e,times}
\usepackage{listings}
\usepackage{clrscode}
\usepackage[table]{xcolor}
\usepackage{array}
\usepackage{slashbox}
\usepackage{multirow}
%\usepackage[cm]{fullpage}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{stfloats}
\usepackage{float}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}
\newtheorem{theo}{Theorem}    % numérotés par section
\newtheorem{lemma}{Lemma}

\newcommand{\RR}{\mathbb R}
\newcommand{\NN}{\mathbb N}
\newcommand{\pkg}[1]{\texttt{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}


\newfloat{Algorithm}{thp}{lop}
\floatname{Algorithm}{Algorithm}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\begin{document}
\nipsfinalcopy
\renewcommand{\arraystretch}{1.5}

\definecolor{lightgray}{rgb}{0.9,0.9,0.9}
\definecolor{pastelblue}{RGB}{213,229,255}
\newcolumntype{a}{>{\columncolor{lightgray}}c}

\title{Support vector comparison machines}

\author{
Toby Dylan Hocking \\
Department of Computer Science\\
Tokyo Institute of Technology, Japan \\
\texttt{toby@sg.cs.titech.ac.jp} \\
\And
Supaporn Spanurattana \\
Department of Computer Science\\
Tokyo Institute of Technology, Japan \\
\texttt{supaporn@sg.cs.titech.ac.jp} \\
\And
Masashi Sugiyama \\
Department of Computer Science\\
Tokyo Institute of Technology, Japan \\
\texttt{sugi@cs.titech.ac.jp} 
}

\maketitle

\begin{abstract}
  In ranking problems, the goal is to learn a ranking function
  $r(x)\in\RR$ from labeled pairs $x,x'$ of input points. In this
  paper, we consider the related comparison problem, where the label
  $y\in\{-1,0,1\}$ indicates which element of the pair is better, or
  if there is no significant difference. We cast the learning problem
  as a margin maximization, and show that it can be solved by
  converting it to a standard SVM.
\end{abstract}

\section{Introduction}

In this paper we consider the supervised comparison problem. Assume
that we have $n$ labeled training pairs and for each pair
$i\in\{1,\dots,n\}$ we have input features $x_i,x_i'\in\RR^p$ and a
label $y_i\in\{-1,0,1\}$ that indicates which element is better:
\begin{equation}
  \label{eq:z}
  y_i =
  \begin{cases}
    -1 & \text{ if $x_i$ is better than $x'_i$ 
      $\Rightarrow$ inequality constraint $r(x_i) > r(x_i')$
    },\\
    0 & \text{ if $x_i$ is as good as $x'_i$
      $\Rightarrow$ equality constraint $r(x_i) = r(x_i')$
    },\\
    1 & \text{ if $x'_i$ is better than $x_i$
      $\Rightarrow$ inequality constraint $r(x_i) < r(x_i')$
    }.
  \end{cases}
\end{equation}
Comparison data naturally arise when considering subjective human
evaluations of pairs of items. For example, if I were to compare some
pairs of movies I have watched, I would say \textit{Les Mis\'erables}
is better than \textit{Star Wars}, and \textit{The Empire Strikes
  Back} is as good as \textit{Star Wars}. Features $x_i,x_i'$ of the
movies can be length in minutes, year of theatrical release,
indicators for genre, actors/actresses, directors, etc.  The goal of
learning is to find a comparison function $c:\RR^p \times \RR^p
\rightarrow \{-1,0,1\}$ which minimizes the zero-one loss $e(\hat y,
y) = I(\hat y \neq y)$ on a test set of data:
\begin{equation}
  \label{eq:min_c}
  \minimize_{c} 
  \sum_{i\in\text{test}}
  e\left[ c(x_i, x_i'), y_i \right].
\end{equation}

% The rest of this article is organized as follows. In
% Section~\ref{sec:related} we discuss links with related work on
% classification and ranking, then in Section~\ref{sec:svm-compare} we
% propose a new algorithm for comparison. We show results on some toy
% data in Section~\ref{sec:results} and discuss future work in
% Section~\ref{sec:conclusions}.

\section{Related work}
\label{sec:related}

\begin{table}[b!]
  \centering
  \begin{tabular}{|a|c|c|}\hline
    \rowcolor{lightgray}
    \backslashbox{Outputs}{Inputs}
    &single items $x$&pairs $x,x'$\\ \hline
    $y\in\{-1,1\}$ &SVM  & SVMrank \citep{ranksvm}   	\\ \hline 
    $y\in\{-1,0,1\}$ &Reject option \citep{reject-option}& this work\\ \hline
  \end{tabular}
  \caption{\label{tab:related} Summary of related max-margin learning methods.}
\end{table}

Comparison is similar to ranking and classification with a reject
option (Table~\ref{tab:related}).  \citet{reject-option} studied the
statistical properties of a hinge loss for an extension of SVM where
$y\in\{-1,0,1\}$, and 0 signifies ``rejection'' or ``no guess,''
calling this the ``classification with reject option'' problem.  There
are many algorithms for the supervised learning to rank problem
\citep{learning-to-rank}, which is similar to the supervised
comparison problem we consider in this paper. The main idea of
learning to rank is to train on labeled pairs of possible documents
$x_i,x_i'$ for the same search query. The labels are $y_i\in\{-1,1\}$,
where $y_i=1$ means document $x_i'$ is more relevant than $x_i$ and
$y_i=-1$ means the opposite. \citet{ranksvm} proposed the SVMrank
algorithm for this problem, and the algorithm we propose here is
similar. The difference is that we also consider the case where both
possible items/documents are judged to be equally good ($y_i=0$).
\citet{rank-with-ties} called this the ``ranking with ties'' problem,
proposed a boosting algorithm, and observed that modeling ties is more
effective when there are more output values. \citet{trueskill}
proposed TrueSkill, a generalization of the Elo chess rating system
which can be applied to comparison data. Neither TrueSkill nor SVMrank
can predict ties ($y_i=0$), and \citet{rank-with-ties} did not respond
to a request for their code, which is what motivates us to propose a
new algorithm.

% \begin{figure}
%   \centering
%   \input{figure-norm-data}
%   \vskip -0.5cm
%   \caption{Geometric interpretation of the comparison
%     problem. \textbf{Top}: input feature pairs $x_i,x_i'\in\RR^p$ are
%     drawn as segments and arrows, colored using the labels
%     $y_i\in\{-1,0,1\}$. The level curves of the underlying ranking
%     function $r(x)=||x||_2^2$ are drawn in grey, and differences
%     $|r(x)-r(x')|\leq 1$ are considered insignificant
%     ($y_i=0$). \textbf{Middle}: in the enlarged feature space, the
%     ranking function is linear: $r(x)=w^\intercal
%     \Phi(x)$. \textbf{Bottom}: two symmetric hyperplanes
%     $w^\intercal[\Phi(x_i')-\Phi(x_i)]\in\{-1,1\}$ are used as a
%     comparison function to separate the difference vectors.}
%   \label{fig:geometry}
% \end{figure}

% \subsection{SVMrank for comparing}

% In this section we explain how to apply the existing SVMrank algorithm
% of \citet{ranksvm} to a comparison data set.

% The goal of the SVMrank algorithm is to learn a ranking function
% $r:\RR^p \rightarrow \RR$. When $r(x)=w^\intercal x$ is linear, the
% primal problem for some cost parameter $C\in\RR^+$ is the following
% quadratic program (QP):
% \begin{equation}
%   \begin{aligned}
%     \minimize_{w, \xi}\ \  & \frac 1 2 w^\intercal w 
%     + C \sum_{i\in I_1\cup I_{-1}} \xi_i \\
%     \text{subject to}\ \  & 
%     \forall i\in I_1\cup I_{-1},\ \xi_i \geq 0,\\
% &    \text{and }\xi_i \geq 1-w^\intercal(x_i'-x_i)y_i,
%   \end{aligned}
%   \label{eq:svmrank}
% \end{equation}
% where $I_y=\{i\mid y_i=y\}$ are the sets of indices for the different
% labels. Note that the equality pairs $i$ such that $y_i=0$ are not
% used in the optimization problem.

% After obtaining an optimal $w\in\RR^p$ by solving (\ref{eq:svmrank}),
% we define a comparison function $c_t:\RR^p\times \RR^p\rightarrow
% \{-1, 0, 1\}$ for any threshold $t\in\RR^+$:
% \begin{equation}
%   \label{eq:svmrank_c_t}
%   c_t(x, x') =
%   \begin{cases}
%     -1 & \text{ if } w^\intercal(x' - x) < -t, \\
%     0 & \text{ if } |w^\intercal(x' - x)| \leq t, \\
%     1 & \text{ if } w^\intercal(x' - x) > t. \\
%   \end{cases}
% \end{equation}
% We can then use grid search to estimate an optimal threshold $\hat t$,
% by minimizing the zero-one loss with respect to all the training
% pairs:
% \begin{equation}
%   \hat t = \argmin_{t}
%   \sum_{i=1}^n
%   e\left[ c_t(x_i, x_i'), y_i \right].
% \end{equation}
% However, there are two potential problems with the learned comparison
% function $c_{\hat t}$. First, the equality pairs $i\in I_0$ are not
% used to learn the weight vector $w$ in (\ref{eq:svmrank}). Second, the
% threshold $\hat t$ is learned in a separate optimization step, which
% may be suboptimal. In the next section, we propose a new algorithm
% that fixes these potential problems.

\section{Support vector comparison machines}
\label{sec:svm-compare}

In this section we discuss new learning algorithms for comparison
problems. In all cases, we will first learn a ranking function
$r:\RR^p\rightarrow\RR$ and then make a prediction with a comparison
function $c:\RR^p\times\RR^p\rightarrow\{-1,0,1\}$, defined in terms
of a threshold function $t:\RR\rightarrow\{-1,0,1\}$:
\begin{equation}
  \label{eq:compare_threshold}
  c(x, x') =
  t\left[
    r(x')-r(x)
  \right]
  =
  \begin{cases}
    -1 & \text{if } r(x') - r(x) < -1, \\
    0 & \text{if } |r(x') - r(x)| \leq 1, \\
    1 & \text{if } r(x') - r(x) > 1. \\
  \end{cases}
\end{equation}

\subsection{LP and QP for separable data}
\label{sec:lp-qp}

To illustrate the nature of the max-margin comparison problem, in this
section we assume that the training data are linearly separable. 
%Later
%in Section~\ref{sec:kernelized-qp}, we discuss how to use the kernel
%trick and a standard SVM solver for learning nonlinear patterns.
Using the following linear program (LP), we learn a linear ranking
function $r(x)=w^\intercal x$ that maximizes the geometric margin
$\mu$. As shown in the left panel of Figure~\ref{fig:hard-margin}, the
geometric margin $\mu$ is the smallest distance from any difference
vector $x_i'-x_i$ to a decision boundary $r(x)\in\{-1,1\}$. Letting
$I_y=\{i\mid y_i=y\}$ be the sets of indices for the different labels,
the max margin LP is
\begin{equation}
  \label{eq:max-margin-lp}
  \begin{aligned}
    \maximize_{\mu\in\RR, w\in\RR^p}\ & \mu \\
    \text{subject to}\ & \mu \leq 1-|w^\intercal (x_i' - x_i)|,\
    \forall\  i\in I_0,\\
    &\mu \leq -1 +  w^\intercal(x_i'-x_i)y_i,\ \forall\ i\in I_1\cup I_{-1}.
  \end{aligned}
\end{equation}
Note that the optimal $\mu>0$ if and only if the data are linearly
separable.

\begin{figure*}[b!]
  \hskip -1.5cm
  \begin{small}
      \input{figure-hard-margin}
  \end{small}
  \vskip -0.9cm
  \caption{The separable LP and QP comparison problems. \textbf{Left}:
    the difference vectors $x'-x$ of the original data and the optimal
    solution to the LP (\ref{eq:max-margin-lp}). \textbf{Middle}: for
    the unscaled flipped data $\tilde x'-\tilde x$ (\ref{eq:tilde}),
    the LP is not the same as the QP
    (\ref{eq:max-margin-qp-tilde}). \textbf{Right}: in these scaled
    data, the QP is equivalent to the LP.}
  \label{fig:hard-margin}
\end{figure*}

A different formulation of the max-margin comparison problem is to
first perform a change of variables, and then solve a QP to learn a
binary SVM. The idea is to maximize the margin between significant
differences $y_i\in\{-1,1\}$ and equality pairs
$y_i=0$. Let~$X_y,X_y'$~be the $|I_y|\times p$ matrices formed by all
the pairs $i\in I_y$. We define a ``flipped'' data set with
$m=|I_1|+|I_{-1}|+2|I_0|$ pairs suitable for training a binary SVM:
\begin{equation}
\label{eq:tilde}
  \tilde X = \left[
    \begin{array}{c}
      X_1 \\
      X_{-1}'\\
      X_0\\
      X_0'
    \end{array}
  \right],\ 
  \tilde X' = \left[
    \begin{array}{c}
      X_1' \\
      X_{-1}\\
      X_0'\\
      X_0
    \end{array}
  \right],\ 
  \tilde y = \left[
    \begin{array}{c}
      1_{|I_1|} \\
      1_{|I_{-1}|}\\
      -1_{|I_0|}\\
      -1_{|I_0|}
    \end{array}
  \right],
\end{equation}
where $1_n$ is an $n$-vector of ones, $\tilde X,\tilde
X'\in\RR^{m\times p}$ and $\tilde y\in\{-1,1\}^m$. Note that $\tilde
y_i=-1$ implies no significant difference between $\tilde x_i$ and
$\tilde x_i'$, and $\tilde y_i=1$ implies that $\tilde x_i'$ is better
than $\tilde x_i$. We then learn an affine function
$f(x)=\beta+u^\intercal x$ using hard-margin binary SVM
\begin{equation}
  \label{eq:max-margin-qp-tilde}
  \begin{aligned}
    \minimize_{u\in\RR^p, \beta\in\RR}\ & u^\intercal u  \\
    \text{subject to}\ & 
    \tilde y_i (\beta + u^\intercal( \tilde x_i'-\tilde x_i) ) \geq 1,
    \ \forall i\in\{1,\dots,m\},
  \end{aligned}
\end{equation}
and define the learned ranking function as $r(x) = -u^\intercal
x/\beta$. The solution to this QP is drawn in black in
Figure~\ref{fig:hard-margin}, and we observed that it is equivalent to
the LP (\ref{eq:max-margin-lp}) when the inputs are scaled.
\subsection{Kernelized QP for non-separable data}
\label{sec:kernelized-qp}
In this section, we assume the data are not separable, and want to
learn a general nonlinear ranking function. Using a positive
definite kernel $\kappa:\RR^p\times \RR^p\rightarrow\RR$,
%  which
% implicitly defines features $\Phi(x)$. As in
% (\ref{eq:max-margin-qp-tilde}), we learn a function $f(x)=\beta +
% u^\intercal \Phi(x)$ which is affine in the feature space. Let
% $\alpha,\alpha'\in\RR^m$ be coefficients such that $u=\sum_{i=1}^m
% \alpha_i \Phi(\tilde x_i) + \alpha_i' \Phi(\tilde x_i')$, and so we
% have
% %the learned function is thus
%we use soft-margin binary SVM to learn an affine function $f(x) =\beta
%+ \sum_{i=1}^m \alpha_i \kappa(\tilde x_i, x) + \alpha_i'
%\kappa(\tilde x_i', x)$, and 
%
we define a nonlinear ranking function
\begin{equation}
  \label{eq:kernelized_r}
  r(x)
%= \frac{u^\intercal \Phi(x)}{-\beta} 
= \frac{1}{-\beta}
  \sum_{i=1}^m
    \alpha_i \kappa(\tilde x_i, x) + \alpha_i'  \kappa(\tilde x_i', x).
\end{equation}

% Let $K=[K_1\cdots K_m\ K_1'\cdots K_m']\in\RR^{2m\times 2m}$ be the
% kernel matrix, where the kernel vectors $K_i,K_i'\in\RR^{2m}$ for all
% $i\in\{1, \dots, m\}$.
% \begin{eqnarray}
%   K_i &=& \left[
%     \begin{array}{cccccc}
%       \kappa(\tilde x_1, \tilde x_i)&
%       \cdots&
%       \kappa(\tilde x_m, \tilde x_i)&
%       \kappa(\tilde x_1', \tilde x_i)&
%       \cdots&
%       \kappa(\tilde x_m', \tilde x_i)
%     \end{array}
%   \right]^\intercal \\
%   K_i' &=& \left[
%     \begin{array}{cccccc}
%       \kappa(\tilde x_1, \tilde x_i')&
%       \cdots&
%       \kappa(\tilde x_m, \tilde x_i')&
%       \kappa(\tilde x_1', \tilde x_i')&
%       \cdots&
%       \kappa(\tilde x_m', \tilde x_i')
%     \end{array}
%   \right]^\intercal.
% \end{eqnarray}
% Letting $a=[\alpha^\intercal\
% \alpha'^\intercal]^\intercal\in\RR^{2m}$, the norm of the linear
% function in the feature space is $w^\intercal w = a^\intercal K a$,
% and we can write the dual soft-margin comparison QP for some
% $C\in\RR^+$ as
% \begin{equation}
%   \begin{aligned}
%       \minimize_{a\in\RR^{2m},\xi\in\RR^m,\beta\in\RR}\ \ & 
%       \frac 1 2 a^\intercal K a + C\sum_{i=1}^m \xi_i \\
%       \text{subject to}\ \ & 
%       \text{for all $i\in\{1,\dots,m\}$, }
%       \xi_i \geq 0,\\
%       &\text{and }
%       \xi_i \geq 1-\tilde y_i(\beta + a^\intercal (K_i'-K_i)).
%   \end{aligned}
% \end{equation}
% Let $\lambda, v\in\RR^m$ be the dual variables, let $Y=\Diag(\tilde
% y)$ be the diagonal matrix of $m$ labels. Then the Lagrangian can be
% written as
% \begin{equation}
%   \label{eq:lagrangian}
%   \mathcal L = \frac 1 2 a^\intercal K a + C\xi^\intercal 1_{m}\\
%   -\lambda^\intercal \xi + v^\intercal(1_m - \tilde y\beta - Y M^\intercal K a - \xi),
% \end{equation}
% where $M=[-I_m \, I_m]^\intercal\in\{-1,0,1\}^{2m\times m}$. Solving
% $\nabla_a \mathcal L=0$ results in the following stationary condition:
% \begin{equation}
%   \label{eq:stationarity}
%   a = M Y v.
% \end{equation}
% The rest of the derivation of the dual comparison problem is the same
% as for the standard binary SVM. The resulting dual QP is
% \begin{equation}
%   \begin{aligned}
%     \label{eq:svm-dual}
%     \minimize_{v\in\RR^m}\ \ &
%     \frac 1 2 v^\intercal Y M^\intercal K M Y v - v^\intercal 1_m\\
%     \text{subject to}\ \ &
%     \sum_{i=1}^m v_i \tilde y_i = 0,\\
% &    \text{for all $i\in\{1,\dots,m\}$, } 0\leq v_i\leq C,
%   \end{aligned}
% \end{equation}
We then use an argument analogous to the one \citet{ranksvm} used to
derive the SVMrank algorithm. It can be shown that learning
(\ref{eq:kernelized_r}) is equivalent to a standard binary SVM with
labels $\tilde y=\{-1,1\}^m$ and kernel $\tilde K = M^\intercal K
M\in\RR^{m\times m}$, where $M=[-I_m \,
I_m]^\intercal\in\{-1,0,1\}^{2m\times m}$ and $K\in\RR^{2m\times 2m}$.
So we can use any efficient SVM solver, such as libsvm. We used the R
interface in the \texttt{kernlab} package, and our code is available
in the \texttt{rankSVMcompare} package on Github.

The overall training procedure has two hyper-parameters to tune: the
cost $C\in\RR^+$ and the kernel $\kappa$. As with standard SVM for
binary classification, these parameters can be tuned by minimizing the
prediction error (\ref{eq:min_c}) on a held-out validation set.

% \begin{algorithm}[b!]
%    \caption{\proc{SVMcompare}}
%    \label{alg:SVMcompare}
% \begin{algorithmic}
%   \STATE {\bfseries Input:} cost $C\in\RR^+$, kernel
%   $\kappa:\RR^p\times \RR^p \rightarrow \RR$, features $X,X'\in\RR^{n \times p}$,
%   labels $y\in\{-1,0,1\}^n$.

%   \STATE \makebox[0.5cm]{$\tilde X$} $\gets [$
%   \makebox[1cm]{$X_1^\intercal$}
%   \makebox[1cm]{$X_{-1}'^\intercal$}
%   \makebox[1cm]{$X_0^\intercal$}
%   \makebox[1cm]{$X_0'^\intercal$}
%   $]^\intercal$.

%   \STATE \makebox[0.5cm]{$\tilde X'$} $\gets [$
%   \makebox[1cm]{$X_1'^\intercal$}
%   \makebox[1cm]{$X_{-1}^\intercal$}
%   \makebox[1cm]{$X_0'^\intercal$}
%   \makebox[1cm]{$X_0^\intercal$}
%   $]^\intercal$.

%   \STATE \makebox[0.5cm]{$\tilde y$} $\gets [$
%   \makebox[1cm]{$1_{|I_1|}^\intercal$}
%   \makebox[1cm]{$1_{|I_{-1}|}^\intercal$}
%   \makebox[1cm]{$-1_{|I_0|}^\intercal$}
%   \makebox[1cm]{$-1_{|I_0|}^\intercal$}
%   $]^\intercal$.

%   \STATE $K \gets \proc{KernelMatrix}(\tilde X, \tilde X', \kappa)$.

%   \STATE $M \gets [ -I_m\ I_m ]^\intercal$.

%   \STATE $\tilde K \gets M^\intercal K M$.

%   \STATE $v,\beta \gets \proc{SVMdual}(\tilde K, \tilde y, C)$.

%   % \STATE $\alpha_i \gets $ 
%   % \makebox[1cm][r]{$-\tilde y_i v_i$},
%   % $\forall i\in\{1,\dots, m\}$.

%   % \STATE $\alpha_i' \gets $
%   % \makebox[1cm][r]{$\tilde y_i v_i$},
%   % $\forall i\in\{1,\dots, m\}$.

%   \STATE $\textbf{sv}\gets\{i: v_i>0\}$.
  
%   \STATE {\bfseries Output:} Support vectors $\tilde
%   X_{\textbf{sv}},\tilde X_{\textbf{sv}}'$, labels $\tilde y_{\textbf{sv}}$,
%   bias~$\beta$, dual variables $v$.

%    \end{algorithmic}
% \end{algorithm}

\section{Results}
\label{sec:results}

We used a simulation to compare the model of
Section~\ref{sec:kernelized-qp} with a baseline ranking model that
ignores the equality $y_i=0$ pairs, SVMrank \citep{ranksvm}. The goal
of our simulation is to demonstrate that our model can perform better
by learning from the equality $y_i=0$ pairs, when there are few
inequality $y_i\in\{-1,1\}$ pairs. We generated pairs
$x_i,x_i'\in[-3,3]^2$ and noisy labels
$y_i=t[r(x'_i)-r(x_i)+\epsilon_i]$, where $t$ is the threshold
function (\ref{eq:compare_threshold}), $r$ is the latent ranking
function, and $\epsilon_i\sim N(0, 1/4)$ is noise. We picked train,
validation, and test sets, each with $n/2$ equality pairs and $n/2$
inequality pairs, for $n\in\{50,\dots,300\}$. We fit a $10\times 10$
grid of models to the training set ($C=10^{-3},\dots,10^3$, Gaussian
kernel width $\gamma=2^{-7},\dots,2^4$), select the model with minimal
zero-one loss on the validation set, and then use the test set to
estimate the generalization ability of the selected model. For the
SVMrank model, equality pairs are ignored when learning the ranking
function, but used to learn a threshold via grid search for when to
predict $c(x,x')=0$.

\begin{figure}[b!]
  \hskip -1cm
  \input{figure-simulation-samples}
  \vskip -0.7cm
  \caption{Application to simulated patterns $x,x'\in\RR^2$. The three
    ranking functions $r(x)=||x||^2$ are squared norms. In each case,
    half of the training data were equality pairs, which are used
    directly by the comparison model, but are ignored by the ranking
    model. We plot mean and standard error of the prediction error
    across 4 randomly chosen data sets, as a function of training set
    size $n$. It is clear that the comparison model makes better
    predictions on the test data.}
  \label{fig:simulation-samples}
\end{figure}

% The model with minimal zero-one loss on the validation set is plotted in
% Figure~\ref{fig:norms}. The comparison model is better at recovering
% the underlying ranking function, since it is able to directly learn
% from the equality pairs ($y_i=0$).

% \begin{figure}[b!]
%   \centering
%   \input{figure-simulation}
%   \vskip -0.5cm
%   \caption{Application to simulated patterns $x,x'\in\RR^2$. The three
%     latent ranking functions $r(x)=||x||^2$ are squared norms. In each
%     case, half of the training data were equality pairs, which are
%     used directly by the comparison model, but are ignored by the
%     ranking model.  We draw the level curves of the learned ranking
%     functions.  It is clear that the comparison model recovers the
%     latent pattern better than the ranking model.}
%   \label{fig:norms}
% \end{figure}

% \begin{table}[b!]
%   \centering
%   \begin{tabular}{|a|c|c|c|}\hline
%     \rowcolor{lightgray}
%     \backslashbox{$\hat{y}$}{ $y$}
%     &\textbf{-1}&\textbf{0}&\textbf{1}\\ \hline
%     \textbf{-1}&0  & FP & Inversion   	\\ \hline 
%     \textbf{0} &FN& 0& FN\\ \hline
%     \textbf{1} & Inversion & FP &0	\\ \hline
%   \end{tabular}
%   % \cellcolor{pastelblue}
%   \caption{We use the zero-one loss to evaluate a predicted label
%     $\hat y$ given the true label $y$. False positives (FP) occur 
%     when predicting a significant difference $\hat y\in\{-1,1\}$ 
%     when there is none $y=0$, and False Negatives (FN) are the opposite.
%   Inversions occur when predicting the opposite of the true label
%   $\hat y = -y$.}
%   \label{tab:evaluation}
% \end{table}

\section{Conclusions and future work}
\label{sec:conclusions}

We discussed an extension of SVM to comparison problems. Our results
highlighted the importance of directly modeling the equality pairs
($y_i=0$), and it will be interesting to see if the same results are
observed in learning to rank data sets. For scaling to very large data
sets, it will be interesting to try algorithms based on smooth
discriminative loss functions, such as stochastic gradient descent
with a logistic loss.

\textbf{Acknowledgements}: TDH was funded by KAKENHI 23120004, SS by a
MEXT scholarship, and MS by KAKENHI 26700022. Thanks to Simon
Lacoste-Julien and Hang Li for helpful discussions.

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
